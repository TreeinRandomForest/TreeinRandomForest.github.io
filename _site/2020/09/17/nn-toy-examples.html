
<p>ESTIMATED TOTAL MEMORY USAGE: 2700 MB (but peaks will hit ~20 GB)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">autoreload</span>
<span class="o">%</span><span class="n">autoreload</span> <span class="mi">2</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span><span class="p">,</span> <span class="n">MLPRegressor</span>

<span class="kn">import</span> <span class="nn">copy</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda:0"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="goals-of-this-notebook">Goals of this notebook</h1>

<p>We want to introduce the basics of neural networks and deep learning. Modern deep learning is a huge field and it’s impossible to cover even all the significant developments in the last 5 years here. But the basics are straightforward.</p>

<p>One big caveat: deep learning is a rapidly evolving field. There are new developments in neural network architectures, novel applications, better optimization techniques, theoretical results justifying why something works etc. daily. It’s a great opportunity to get involved if you find research interesting and there are great online communities (pytorch, fast.ai, paperswithcode, pysyft) that you should get involved with.</p>

<p><strong>Note</strong>: Unlike the previous notebooks, this notebook has very few questions. You should study the code, tweak the data, the parameters, and poke the models to understand what’s going on.</p>

<p><strong>Notes</strong>: You can install extensions (google for nbextensions) with Jupyter notebooks. I tend to use resuse to display memory usage in the top right corner which really helps.</p>

<p>To run a cell, press: “Shift + Enter”</p>

<p>To add a cell before your current cell, press: “Esc + a”</p>

<p>To add a cell after your current cell, press: “Esc + b”</p>

<p>To delete a cell, press: “Esc + x”</p>

<p>To be able to edit a cell, press: “Enter”</p>

<p>To see more documentation about of a function, type ?function_name</p>

<p>To see source code, type ??function_name</p>

<p>To quickly see possible arguments for a function, type “Shift + Tab” after typing the function name.</p>

<p>Esc and Enter take you into different modes. Press “Esc + h” to see all shortcuts.</p>

<h2 id="syntheticartificial-datasets">Synthetic/Artificial Datasets</h2>

<p>We covered the basics of neural networks in the lecture. We also saw applications to two synthetic datasets. The goal in this section is to replicate those results and get a feel for using pytorch.</p>

<h3 id="classification">Classification</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_binary_data</span><span class="p">(</span><span class="n">N_examples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">features</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">target</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_examples</span><span class="p">):</span>
        <span class="c">#class = 0</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>

        <span class="n">features</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">r</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">r</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)])</span>
        <span class="n">target</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c">#class = 1</span>
        <span class="n">r</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>

        <span class="n">features</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">r</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">r</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)])</span>
        <span class="n">target</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">features</span><span class="p">,</span> <span class="n">target</span>    
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">features</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">generate_binary_data</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_binary_data</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">features</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="mi">1</span><span class="p">],</span> <span class="s">'p'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'0'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">features</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="mi">1</span><span class="p">],</span> <span class="s">'p'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'g'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'1'</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'y'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_binary_data</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</code></pre></div></div>

<p>We have two features here - x and y. There is a binary target variable that we need to predict. This is essentially the dataset from the logistic regression discussion. Logistic regression will not do well here given that the data is not linearly separable. Transforming the data so we have two features:</p>

<script type="math/tex; mode=display">r^2  = x^2 + y^2</script>

<p>and</p>

<script type="math/tex; mode=display">\theta = \arctan(\frac{y}{x})</script>

<p>would make it very easy to use logistic regression (or just a cut at $r = 2$) to separate the two classes but while it is easy for us to visualize the data and guess at the transformation, in high dimensions, we can’t follow the same process.</p>

<p>Let’s implement a feed-forward neural network that takes the two features as input and predicts the probabiliy of being in class 1 as output.</p>

<h4 id="architecture-definition">Architecture Definition</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ClassifierNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span> <span class="c">#inherit from nn.Module to define your own architecture</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N_inputs</span><span class="p">,</span> <span class="n">N_outputs</span><span class="p">,</span> <span class="n">N_hidden_layers</span><span class="p">,</span> <span class="n">N_hidden_nodes</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">output_activation</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ClassifierNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">N_inputs</span> <span class="o">=</span> <span class="n">N_inputs</span> <span class="c">#2 in our case</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N_outputs</span> <span class="o">=</span> <span class="n">N_outputs</span> <span class="c">#1 in our case but can be higher for multi-class classification</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">N_hidden_layers</span> <span class="o">=</span> <span class="n">N_hidden_layers</span> <span class="c">#we'll start by using one hidden layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N_hidden_nodes</span> <span class="o">=</span> <span class="n">N_hidden_nodes</span> <span class="c">#number of nodes in each hidden layer - can extend to passing a list</span>
        
        <span class="c">#Define layers below - pytorch has a lot of layers pre-defined</span>
        
        <span class="c">#use nn.ModuleList or nn.DictList instead of [] or {} - more explanations below</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_list</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([])</span> <span class="c">#use just as a python list</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_hidden_layers</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">n</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layer_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">N_inputs</span><span class="p">,</span> <span class="n">N_hidden_nodes</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layer_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">N_hidden_nodes</span><span class="p">,</span> <span class="n">N_hidden_nodes</span><span class="p">))</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">N_hidden_nodes</span><span class="p">,</span> <span class="n">N_outputs</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span> <span class="c">#activations at inner nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_activation</span> <span class="o">=</span> <span class="n">output_activation</span> <span class="c">#activation at last layer (depends on your problem)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span>
        <span class="s">'''
        every neural net in pytorch has its own forward function
        this function defines how data flows through the architecture from input to output i.e. the forward propagation part
        '''</span>
        
        <span class="n">out</span> <span class="o">=</span> <span class="n">inp</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_list</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="c">#calls forward function for each layer (already implemented for us)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="c">#non-linear activation</span>
            
        <span class="c">#pass activations through last/output layer</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_activation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_activation</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">out</span>
        
        <span class="k">return</span> <span class="n">pred</span>
</code></pre></div></div>

<p>There are several ways of specifying a neural net architecture in pytorch. You can work at a high level of abstraction by just listing the layers that you want to getting into the fine details by constructing your own layers (as classes) that can be used in ClassifierNet above.</p>

<p>How does pytorch work? When you define an architecture like the one above, pytorch constructs a graph (nodes and edges) where the nodes are operations on multi-indexed arrays (called tensors).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N_inputs</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">N_outputs</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">N_hidden_layers</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">N_hidden_nodes</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="n">output_activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span> <span class="c">#we want one probability between 0-1</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">ClassifierNet</span><span class="p">(</span><span class="n">N_inputs</span><span class="p">,</span>
                    <span class="n">N_outputs</span><span class="p">,</span>
                    <span class="n">N_hidden_layers</span><span class="p">,</span>
                    <span class="n">N_hidden_nodes</span><span class="p">,</span>
                    <span class="n">activation</span><span class="p">,</span>
                    <span class="n">output_activation</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="training">Training</h4>

<p><strong>Loss function</strong></p>

<p>We first need to pick our loss function. Like we binary classification problems (including logistic regression), we’ll use binary cross-entropy:</p>

<script type="math/tex; mode=display">\text{Loss, } L = -\Sigma_{i=1}^{N} y_i \log(p_i) + (1-y_i) \log(1-p_i)</script>

<p>where $y_i \in {0,1}$ are the labels and $p_i \in [0,1]$ are the probability predictions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#look at all available losses (you can always write your own)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.*</span><span class="n">Loss</span><span class="err">?</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#get a feel for the loss function</span>
<span class="c">#target = 1 (label = 1)</span>
<span class="k">print</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">)))</span> <span class="c">#pred prob = 1e-2 -&gt; BAD</span>
<span class="k">print</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">)))</span> <span class="c">#pred prob = 0.3 -&gt; BAd</span>
<span class="k">print</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">)))</span> <span class="c">#pred prob = 0.5 -&gt; Bad</span>
<span class="k">print</span><span class="p">(</span><span class="n">criterion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">)))</span> <span class="c">#pred prob = 1.0 -&gt; GREAT!</span>
</code></pre></div></div>

<p><strong>Optimizer</strong>:</p>

<p>So we have the data, the neural net architecture, a loss function to measure how well the model does on our task. We also need a way to do gradient descent.</p>

<p>Recall, we use gradient descent to minimize the loss by computing the first derivative (gradients) and taking a step in the direction opposite (since we are minimizing) to the gradient:</p>

<script type="math/tex; mode=display">w_{t} \rightarrow w_{t} - \eta \frac{\partial L}{\partial w_{t-1}}</script>

<p>where $w_t$ = weight at time-step t, $L$ = loss, $\eta$ = learning rate.</p>

<p>For our neural network, we first need to calculate the gradients. Thankfully, this is done automatically by pytorch using a procedure called <strong>backpropagation</strong>. If you are interested in more calculations details, please check “automatic differentiation” and an analytical calculation for a feed-forward network (https://treeinrandomforest.github.io/deep-learning/2018/10/30/backpropagation.html).</p>

<p>The gradients are calculated by calling a function <strong>backward</strong> on the network, as we’ll see below.</p>

<p>Once the gradients are calculated, we need to update the weights. In practice, there are many heuristics/variants of the update step above that lead to better optimization behavior. A great resource to dive into details is https://ruder.io/optimizing-gradient-descent/. We won’t get into the details here.</p>

<p>We’ll choose what’s called the <strong>Adam</strong> optimizer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optim</span><span class="o">.*</span><span class="err">?</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
</code></pre></div></div>

<p>We picked a constant learning rate here (which is adjusted internally by Adam) and also passed all the tunable weights in the network by using: net.parameters()</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">list</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</code></pre></div></div>

<p>There are 9 free parameters:</p>

<ul>
  <li>
    <p>A 2x2 matrix (4 parameters) mapping the input layer to the 1 hidden layer.</p>
  </li>
  <li>
    <p>A 2x1 matrix (2 parameters) mapping the hidden layer to the output layer with one node.</p>
  </li>
  <li>
    <p>2 biases for the 2 nodes in the hidden layer.</p>
  </li>
  <li>
    <p>1 bias for the output node in the output layer.</p>
  </li>
</ul>

<p>This is a good place to explain why we need to use nn.ModuleList. If we had just used a vanilla python list, net.parameters() would only show weights that are explicitly defined in our net architecture. The weights and biases associated with the layers would NOT show up in net.parameters(). This process of a module higher up in the hierarchy (ClassifierNet) subsuming the weights and biases of modules lower in the hierarchy (layers) is called <strong>registering</strong>. ModuleList ensures that all the weights/biases are registered as weights and biases of ClassifierNet.</p>

<p>Let’s combine all these elements and train our first neural net.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#convert features and target to torch tensors</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#if have gpu, throw the model, features and labels on it</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span>
</code></pre></div></div>

<p>We need to do the following steps now:</p>

<ul>
  <li>
    <p>Compute the gradients for our dataset.</p>
  </li>
  <li>
    <p>Do gradient descent and update the weights.</p>
  </li>
  <li>
    <p>Repeat till ??</p>
  </li>
</ul>

<p>The problem is there’s no way of knowing when we have converged or are close to the minimum of the loss function. In practice, this means we keep repeating the process above and monitor the loss as well as performance on a hold-out set. When we start over-fitting on the training set, we stop. There are various modifications to this procedure but this is the essence of what we are doing.</p>

<p>Each pass through the whole dataset is called an <strong>epoch</strong>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N_epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_epochs</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">features</span><span class="p">)</span> <span class="c">#make predictions on the inputs</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="c">#compute loss on our predictions</span>
    
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c">#set all gradients to 0</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c">#backprop to compute gradients</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c">#update the weights</span>
    
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Loss = {loss:.4f}'</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s combined all these elements into a function</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">N_epochs</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">(),</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="c">#criterion = nn.BCELoss() #binary cross-entropy loss as before</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span> <span class="c">#Adam optimizer</span>

    <span class="c">#if have gpu, throw the model, features and labels on it</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">features</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_epochs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span> <span class="c">#should have no effect on gradients in this case</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">))</span>

            <span class="n">features_shuffled</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
            <span class="n">target_shuffled</span> <span class="o">=</span> <span class="n">target</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">features_shuffled</span> <span class="o">=</span> <span class="n">features</span>
            <span class="n">target_shuffled</span> <span class="o">=</span> <span class="n">target</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">features_shuffled</span><span class="p">)</span>
        <span class="c">#out = out.reshape(out.size(0))</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">target_shuffled</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'epoch = {epoch} loss = {loss}'</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">features_shuffled</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
    <span class="n">pred</span><span class="p">[</span><span class="n">pred</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">pred</span><span class="p">[</span><span class="n">pred</span><span class="o">&lt;=</span><span class="mf">0.5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c">#print(f'Accuracy = {accuracy}')</span>
        
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s">'cpu'</span><span class="p">)</span>
    <span class="n">features</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s">'cpu'</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s">'cpu'</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<p><strong>Exercise</strong>: Train the model and vary the number of hidden nodes and see what happens to the loss. Can you explain this behavior?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N_inputs</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">N_outputs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">N_hidden_layers</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">N_hidden_nodes</span> <span class="o">=</span> <span class="mi">1</span> <span class="c">#&lt;--- play with this</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="n">output_activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span> <span class="c">#we want one probability between 0-1</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">ClassifierNet</span><span class="p">(</span><span class="n">N_inputs</span><span class="p">,</span>
                    <span class="n">N_outputs</span><span class="p">,</span>
                    <span class="n">N_hidden_layers</span><span class="p">,</span>
                    <span class="n">N_hidden_nodes</span><span class="p">,</span>
                    <span class="n">activation</span><span class="p">,</span>
                    <span class="n">output_activation</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N_inputs</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">N_outputs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">N_hidden_layers</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">N_hidden_nodes</span> <span class="o">=</span> <span class="mi">2</span> <span class="c">#&lt;--- play with this</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="n">output_activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span> <span class="c">#we want one probability between 0-1</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">ClassifierNet</span><span class="p">(</span><span class="n">N_inputs</span><span class="p">,</span>
                    <span class="n">N_outputs</span><span class="p">,</span>
                    <span class="n">N_hidden_layers</span><span class="p">,</span>
                    <span class="n">N_hidden_nodes</span><span class="p">,</span>
                    <span class="n">activation</span><span class="p">,</span>
                    <span class="n">output_activation</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N_inputs</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">N_outputs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">N_hidden_layers</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">N_hidden_nodes</span> <span class="o">=</span> <span class="mi">3</span> <span class="c">#&lt;--- play with this</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="n">output_activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span> <span class="c">#we want one probability between 0-1</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">ClassifierNet</span><span class="p">(</span><span class="n">N_inputs</span><span class="p">,</span>
                    <span class="n">N_outputs</span><span class="p">,</span>
                    <span class="n">N_hidden_layers</span><span class="p">,</span>
                    <span class="n">N_hidden_nodes</span><span class="p">,</span>
                    <span class="n">activation</span><span class="p">,</span>
                    <span class="n">output_activation</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
</code></pre></div></div>

<p>There seems to be some “magic” behavior when we increase the number of nodes in the first (and only) hidden layer from 2 to 3. Loss suddenly goes down dramatically. At this stage, we should explore why that’s happening.</p>

<p>For every node in the hidden layer, we have a mapping from the input to that node:</p>

<script type="math/tex; mode=display">\sigma(w_1 x + w_2 y + b)</script>

<p>where $w_1, w_2, b$ are specific to that hidden node. We can plot the decision line in this case:</p>

<script type="math/tex; mode=display">w_1 x + w_2 y + b = 0</script>

<p>Unlike logistic regression, this is not actually a decision line. Points on one side are not classified as 0 and points on the other side as 1 (if the threshold = 0.5). Instead this line should be thought of as one defining a new coordinate-system. Instead of x and y coordinates, every hidden node induces a straight line and a new coordinate, say $\alpha_i$. So if we have 3 hidden nodes, we are mapping the 2-dimensional input space into a 3-dimensional space where the coordinates $\alpha_1, \alpha_2, \alpha_3$ for each point depend on which side of the 3 lines induced as mentioned above, it lies.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c">#3x2 matrix</span>
<span class="k">print</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="c">#3 biases</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">features</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c">#detach from pytorch computational graph, bring back to cpu, convert to numpy</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">()</span>

<span class="c">#plot raw data</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">features</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="mi">1</span><span class="p">],</span> <span class="s">'p'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'0'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">features</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="mi">1</span><span class="p">],</span> <span class="s">'p'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'g'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'1'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'y'</span><span class="p">)</span>

<span class="c">#get weights and biases</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">biases</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="c">#plot straight lines</span>
<span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">features</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span>
<span class="n">y_lim_min</span><span class="p">,</span> <span class="n">y_lim_max</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">features</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span> <span class="c">#loop over each hidden node in the one hidden layer</span>
    <span class="n">coef</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="n">biases</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    
    <span class="n">y_min</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">intercept</span> <span class="o">-</span> <span class="n">coef</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x_min</span><span class="p">)</span><span class="o">/</span><span class="n">coef</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">y_max</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">intercept</span> <span class="o">-</span> <span class="n">coef</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x_max</span><span class="p">)</span><span class="o">/</span><span class="n">coef</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">],</span> <span class="p">[</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">])</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">y_lim_min</span><span class="p">,</span> <span class="n">y_lim_max</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">framealpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>This is the plot we showed in the lecture. For every hidden node in the hidden layer, we have a straight line. The colors of the three lines above are orange, green and blue and that’s what we’ll call our new coordinates.</p>

<p>Suppose you pick a point in the red region:</p>

<ul>
  <li>
    <p>It lies to the <em>right</em> of the orange line</p>
  </li>
  <li>
    <p>It lies to the <em>bottom</em> of the green line</p>
  </li>
  <li>
    <p>It lies to the <em>top</em> of the blue line.</p>
  </li>
</ul>

<p>(These directions might change because of inherent randomness during training - weight initializations here).</p>

<p>On the other hand, we have <strong>6</strong> green regions. If you start walking clockwise from the top green section, every time you cross a straight line, you walk into a new region. Each time you walk into a new region, you flip the coordinate of one of the 3 lines. Either you go from <em>right</em> to <em>left</em> of the orange line, <em>bottom</em> to <em>top</em> of the green line or <em>top</em> to <em>bottom</em> of the blue line.</p>

<p>So instead of describing each point by two coordinates (x, y), we can describe it by (orange status, green status, blue status). We happen to have 7 such regions here - with 1 being purely occupied by the red points and the other 7 by green points.</p>

<p>This might be become cleared from a 3-dimensional plot.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#get hidden layer activations for all inputs</span>
<span class="n">features_layer1_3d</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">layer_list</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">features</span><span class="p">)))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">features_layer1_3d</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="mi">2</span><span class="p">],</span> <span class="s">'p'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'0'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="mi">2</span><span class="p">],</span> <span class="s">'p'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span><span class="s">'g'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'1'</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">framealpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>At this stage, a simple linear classifier can draw a linear decision boundary (a plane) to separate the red points from the green points. Also, these points lie in the unit cube (cube with sides of length=1) since we are using sigmoid activations. Whenever the activations get saturated (close to 0 or 1), then we see points on the edges and corners of the cube.</p>

<p><strong>Question</strong>: Switch the activation from sigmoid to relu (nn.ReLU()). Does the loss still essentially become zero on the train set? If not, try increasing N_hidden_nodes. At what point does the loss actually become close to 0?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N_inputs</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">N_outputs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">N_hidden_layers</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">N_hidden_nodes</span> <span class="o">=</span> <span class="mi">5</span> <span class="c">#&lt;---- play with this</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="n">output_activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span> <span class="c">#we want one probability between 0-1</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">ClassifierNet</span><span class="p">(</span><span class="n">N_inputs</span><span class="p">,</span>
                    <span class="n">N_outputs</span><span class="p">,</span>
                    <span class="n">N_hidden_layers</span><span class="p">,</span>
                    <span class="n">N_hidden_nodes</span><span class="p">,</span>
                    <span class="n">activation</span><span class="p">,</span>
                    <span class="n">output_activation</span><span class="p">)</span>

<span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Question</strong>: Remake the 3d plot but by trying 3 coordinates out of the N_hidden_nodes coordinates you found above?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">features</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c">#detach from pytorch computational graph, bring back to cpu, convert to numpy</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#get hidden layer activations for all inputs</span>
<span class="n">features_layer1_3d</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">layer_list</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">features</span><span class="p">)))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">features_layer1_3d</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>

<span class="n">COORD1</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">COORD2</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">COORD3</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="n">COORD1</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="n">COORD2</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="n">COORD3</span><span class="p">],</span> <span class="s">'p'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'0'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="n">COORD1</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="n">COORD2</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="n">COORD3</span><span class="p">],</span> <span class="s">'p'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span><span class="s">'g'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'1'</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">framealpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>

<span class="n">COORD1</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">COORD2</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">COORD3</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="n">COORD1</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="n">COORD2</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="n">COORD3</span><span class="p">],</span> <span class="s">'p'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'0'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="n">COORD1</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="n">COORD2</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="n">COORD3</span><span class="p">],</span> <span class="s">'p'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span><span class="s">'g'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'1'</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">framealpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>

<span class="n">COORD1</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">COORD2</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">COORD3</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="n">COORD1</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="n">COORD2</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="n">COORD3</span><span class="p">],</span> <span class="s">'p'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'0'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="n">COORD1</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="n">COORD2</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="n">COORD3</span><span class="p">],</span> <span class="s">'p'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span><span class="s">'g'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'1'</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">framealpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>

<span class="n">COORD1</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">COORD2</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">COORD3</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="n">COORD1</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="n">COORD2</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="n">COORD3</span><span class="p">],</span> <span class="s">'p'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'0'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="n">COORD1</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="n">COORD2</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="n">COORD3</span><span class="p">],</span> <span class="s">'p'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span><span class="s">'g'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'1'</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">framealpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>Draw all the plots</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">itertools</span>

<span class="k">for</span> <span class="n">comb</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">combinations</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N_hidden_nodes</span><span class="p">),</span> <span class="mi">3</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>

    <span class="n">COORD1</span> <span class="o">=</span> <span class="n">comb</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">COORD2</span> <span class="o">=</span> <span class="n">comb</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">COORD3</span> <span class="o">=</span> <span class="n">comb</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="n">COORD1</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="n">COORD2</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">0</span><span class="p">][:,</span><span class="n">COORD3</span><span class="p">],</span> <span class="s">'p'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'0'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="n">COORD1</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="n">COORD2</span><span class="p">],</span> <span class="n">features_layer1_3d</span><span class="p">[</span><span class="n">target</span><span class="o">==</span><span class="mi">1</span><span class="p">][:,</span><span class="n">COORD3</span><span class="p">],</span> <span class="s">'p'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span><span class="s">'g'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'1'</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">framealpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">f</span><span class="s">'COORDINATES = {comb}'</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Note</strong>: Generally it is a good idea to use a linear layer for the output layer and use BCEWithLogitsLoss to avoid numerical instabilities. We will do this later for multi-class classification.</p>

<p>Clear variables</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">features</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">features_layer1_3d</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">target</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">net</span> <span class="o">=</span> <span class="bp">None</span>
</code></pre></div></div>

<h3 id="regression">Regression</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_regression_data</span><span class="p">(</span><span class="n">L</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">stepsize</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="n">L</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">stepsize</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span> <span class="o">/</span> <span class="mf">8.</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">plot_regression_data</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'y'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_regression_data</span><span class="p">()</span>
<span class="n">plot_regression_data</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>This is a pretty different problem in some ways. We now have one input - x and one output - y. But looked at another way, we simply change the number of inputs in our neural network to 1 and we change the output activation to be a linear function. Why linear? Because in principle, the output (y) can be unbounded i.e. any real value.</p>

<p>We also need to change the loss function. While binary cross-entropy is appropriate for a classification problem, we need something else for a regression problem. We’ll use mean-squared error:</p>

<script type="math/tex; mode=display">\frac{1}{2}(y_{\text{target}} - y_{\text{pred}})^2</script>

<p>Try modifying N_hidden_nodes from 1 through 10 and see what happens to the loss</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N_inputs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">N_outputs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">N_hidden_layers</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">N_hidden_nodes</span> <span class="o">=</span> <span class="mi">10</span> <span class="c">#&lt;--- play with this</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="n">output_activation</span> <span class="o">=</span> <span class="bp">None</span> <span class="c">#we want one probability between 0-1</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">ClassifierNet</span><span class="p">(</span><span class="n">N_inputs</span><span class="p">,</span>
                    <span class="n">N_outputs</span><span class="p">,</span>
                    <span class="n">N_hidden_layers</span><span class="p">,</span>
                    <span class="n">N_hidden_nodes</span><span class="p">,</span>
                    <span class="n">activation</span><span class="p">,</span>
                    <span class="n">output_activation</span><span class="p">)</span>

<span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mi">20000</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pred</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
</code></pre></div></div>

<p>As before, we need to understand what the model is doing. As before, let’s consider the mapping from the input node to one node of the hidden layer. In this case, we have the mapping:</p>

<script type="math/tex; mode=display">\sigma(w_i x + b_i)</script>

<p>where $w_i, b_i$ are the weight and bias associated with each node of the hidden layer. This defines a “decision” boundary where:</p>

<script type="math/tex; mode=display">w_i x + b_i = 0</script>

<p>This is just a value $\delta_{i} \equiv -\frac{b_i}{w_i}$.</p>

<p>For each hidden node $i$, we can calculate one such threshold, $\delta_i$.</p>

<p>As we walk along the x-axis from the left to right, we will cross each threshold one by one. On crossing each threshold, one hidden node switches i.e. goes from $0 \rightarrow 1$ or $1 \rightarrow 0$. What effect does this have on the output or prediction?</p>

<p>Since the last layer is linear, its output is:</p>

<p>$y = v_1 h_1 + v_2 h_2 + \ldots + v_n h_n + c$</p>

<p>where $v_i$ are the weights from the hidden layer to the output node, $c$ is the bias on the output node, and $h_i$ are the activations on the hidden nodes. These activations can smoothly vary between 0 and 1 according to the sigmoid function.</p>

<p>So, when we cross a threshold, one of the $h_j$ values eithers turns off or turns on. This has the effect of adding or subtracting constant $v_k$ values from the output if the kth hidden node, $h_k$ is switching on/off.</p>

<p>This means that as we add more hidden nodes, we can divide the domain (the x values) into more fine-grained intervals that can be assigned a single value by the neural network. In practice, there is a smooth interpolation.</p>

<p><strong>Question</strong>: Suppose instead of the sigmoid activations, we used a binary threshold:</p>

<script type="math/tex; mode=display">% <![CDATA[
\sigma(x) = \begin{cases}
1 & x > 0 \\
0 & x \leq 0
\end{cases} %]]></script>

<p>then we would get a piece-wise constant prediction from our trained network. Plot that piecewise function as a function of $x$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">activations</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">layer_list</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">features</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">binary_activations</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Threshold</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">)(</span><span class="n">activations</span><span class="p">)</span><span class="o">/</span><span class="n">activations</span>
<span class="k">print</span><span class="p">(</span><span class="n">binary_activations</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">binary_pred</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">output_layer</span><span class="p">(</span><span class="n">binary_activations</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'data'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">binary_pred</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s">'binary'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'pred'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Question</strong>: Why does the left part of the function fit so well but the right side is always compromised? Hint: think of the loss function.</p>

<p>The most likely reason is that the loss function is sensitive to the scale of the $y$ values. A 10% deviation between the y-value and the prediction near x = -10 has a larger absolute value than a 10% deviation near say, x = 5.</p>

<p><strong>Question</strong>: Can you think of ways to test this hypothesis?</p>

<p>There are a couple of things you could do. One is to flip the function from left to right and re-train the model. In this case, the right side should start fitting better.</p>

<p>Another option is to change the loss function to percentage error i.e.:</p>

<script type="math/tex; mode=display">\frac{1}{2} \big(\frac{y_{\text{target}} - y_{\text{pred}}}{y_{\text{target}}}\big)^2</script>

<p>but this is probably much harder to optimize.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">y</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N_inputs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">N_outputs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">N_hidden_layers</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">N_hidden_nodes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="n">output_activation</span> <span class="o">=</span> <span class="bp">None</span> <span class="c">#we want one probability between 0-1</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">ClassifierNet</span><span class="p">(</span><span class="n">N_inputs</span><span class="p">,</span>
                    <span class="n">N_outputs</span><span class="p">,</span>
                    <span class="n">N_hidden_layers</span><span class="p">,</span>
                    <span class="n">N_hidden_nodes</span><span class="p">,</span>
                    <span class="n">activation</span><span class="p">,</span>
                    <span class="n">output_activation</span><span class="p">)</span>

<span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mi">14000</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pred</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
</code></pre></div></div>

<p>As expected, now the right side of the function fits well.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">activations</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">layer_list</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">features</span><span class="p">))</span>
<span class="n">binary_activations</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Threshold</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">)(</span><span class="n">activations</span><span class="p">)</span><span class="o">/</span><span class="n">activations</span>
<span class="n">binary_pred</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">output_layer</span><span class="p">(</span><span class="n">binary_activations</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'data'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">binary_pred</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s">'binary'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'pred'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="clear-memory">Clear Memory</h3>

<p>At this stage, you should restart the kernel and clear the output since we don’t need anything from before.</p>

<h3 id="image-classification">Image Classification</h3>

<p>One of the most successful applications of deep learning has been to computer vision. A central task of computer vision is <strong>image classification</strong>. This is the task of assigning exactly one of multiple labels to an image.</p>

<p>pytorch provides a package called <strong>torchvision</strong> which includes datasets, some modern neural network architectures as well as helper functions for images.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span><span class="p">,</span> <span class="n">MLPRegressor</span>

<span class="kn">import</span> <span class="nn">copy</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda:0"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cuda:0
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">DOWNLOAD_PATH</span> <span class="o">=</span> <span class="s">"../data/MNIST"</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist_train</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">DOWNLOAD_PATH</span><span class="p">,</span> 
                    <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                    <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()]))</span>

<span class="n">mnist_test</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">DOWNLOAD_PATH</span><span class="p">,</span> 
                   <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
                   <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                   <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()]))</span>
</code></pre></div></div>

<p>You will most likely run into memory issues between the data and the weights/biases of your neural network. Let’s instead sample 1/10th the dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">mnist_train</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">mnist_train</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">N_choose</span> <span class="o">=</span> <span class="mi">6000</span>

<span class="n">chosen_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">mnist_train</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">N_choose</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">chosen_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="n">mnist_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">chosen_ids</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">mnist_train</span><span class="o">.</span><span class="n">targets</span><span class="p">[</span><span class="n">chosen_ids</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">mnist_train</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">mnist_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">chosen_ids</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
<span class="n">mnist_train</span><span class="o">.</span><span class="n">targets</span> <span class="o">=</span> <span class="n">mnist_train</span><span class="o">.</span><span class="n">targets</span><span class="p">[</span><span class="n">chosen_ids</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([60000, 28, 28])
torch.Size([60000])
[24002 52251  6159 49269 12283 51361  3932  7264 33050 14980]
torch.Size([6000, 28, 28])
torch.Size([6000])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">mnist_test</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">mnist_test</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">N_choose</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">chosen_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">mnist_test</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">N_choose</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">chosen_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="n">mnist_test</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">chosen_ids</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">mnist_test</span><span class="o">.</span><span class="n">targets</span><span class="p">[</span><span class="n">chosen_ids</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">mnist_test</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">mnist_test</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">chosen_ids</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
<span class="n">mnist_test</span><span class="o">.</span><span class="n">targets</span> <span class="o">=</span> <span class="n">mnist_test</span><span class="o">.</span><span class="n">targets</span><span class="p">[</span><span class="n">chosen_ids</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([10000, 28, 28])
torch.Size([10000])
[8114  643 5401 4385  169 5141 9096 3678 8051   95]
torch.Size([1000, 28, 28])
torch.Size([1000])
</code></pre></div></div>

<p>MNIST is one of the classic image datasets and consists of 28 x 28 pixel images of handwritten digits. We downloaded both the train and test sets. Transforms defined under target_transform will be applied to each example. In this example, we want tensors and not images which is what the transforms do.</p>

<p>The train set consists of 60000 images.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist_train</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([6000, 28, 28])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  58, 109,
          89, 171, 192,  37,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  63, 244, 253,
         254, 253, 254, 243,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  16, 223, 255, 243,
         115,  94, 244, 254, 125,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 171, 253, 191,  36,
           0,   0, 160, 253, 207,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  84, 254,  98,   0,   0,
           0,   0,  37, 253, 217,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 145, 254,  57,   0,   0,
           0,   0,   6, 222, 238,  21,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 145, 255, 212,  21,   0,
           0,  21, 171, 254, 125,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  83, 254, 253, 228, 166,
         145, 228, 243,  98,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  84, 228, 255, 254,
         254, 191,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  63, 254, 253,
         254, 253, 187,  42,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  37, 233, 228,  63,
         135, 238, 254, 233,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  21, 223, 232,  42,   0,
           0,  42, 254, 253,  84,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 135, 249,  52,   0,   0,
           0,   0, 171, 254, 145,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 217, 217,   0,   0,   0,
           0,   0,  68, 253, 166,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,  16, 233, 218,   0,   0,   0,
           0,   0,  21, 238, 218,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   6, 202, 249,  94,   0,   0,
           0,   0,   0, 197, 238,  21,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  94, 254, 233,  42,   0,
           0,   0,  27, 233, 156,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  52, 254, 253, 238, 155,
         135, 135, 213, 253,  73,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 135, 228, 254, 254,
         254, 254, 254, 160,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11,  88, 170,
         181, 201, 139,   5,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],
       dtype=torch.uint8)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">mnist_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.image.AxesImage at 0x7fb450579dd0&gt;
</code></pre></div></div>

<p><img src="output_104_1.png" alt="png" /></p>

<p>There are 10 unique labels - 0 through 9</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist_train</span><span class="o">.</span><span class="n">targets</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([8, 2, 6, 3, 6, 7, 6, 9, 7, 1])
</code></pre></div></div>

<p>The labels are roughly equally/uniformly distributed</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">mnist_train</span><span class="o">.</span><span class="n">targets</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),
 array([612, 649, 594, 635, 580, 521, 591, 640, 557, 621]))
</code></pre></div></div>

<p>The test set consists of 10000 images.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist_test</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([1000, 28, 28])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">mnist_test</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">10</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.image.AxesImage at 0x7fb43bdd1e90&gt;
</code></pre></div></div>

<p><img src="output_111_1.png" alt="png" /></p>

<p>Same labels</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist_test</span><span class="o">.</span><span class="n">targets</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0, 2, 6, 2, 4, 0, 1, 5, 4, 4])
</code></pre></div></div>

<p>Pretty equally distributed.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">mnist_test</span><span class="o">.</span><span class="n">targets</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),
 array([113, 111, 103,  90,  88,  93, 102,  97, 102, 101]))
</code></pre></div></div>

<p><strong>Image Classifier</strong>:</p>

<p>We first have to pick an architecture. The first one we’ll pick is a feed-forward neural network like the one we used in the exercises above. This time I am going to use a higher abstraction to define the network.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#convert 28x28 image -&gt; 784-dimensional flattened vector</span>
<span class="k">class</span> <span class="nc">Flatten</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Flatten</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">inp</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Flatten</span><span class="p">()(</span><span class="n">mnist_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">])</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([10, 784])
</code></pre></div></div>

<p>Architecture definition using nn.Sequential. You can just list the layers in a sequence. We carry out the following steps:</p>

<ul>
  <li>
    <p>Flatten each image into a 784 dimensional vector</p>
  </li>
  <li>
    <p>Map the image to a 100-dimensional vector using a linear layer</p>
  </li>
  <li>
    <p>Apply a relu non-linearity</p>
  </li>
  <li>
    <p>Map the 100-dimensional vector into a 10-dimensional output layer since we have 10 possible targets.</p>
  </li>
  <li>
    <p>Apply a softmax activation to convert the 10 numbers into a probability distribution that assigns the probability the image belonging to each class (0 through 9)</p>
  </li>
</ul>

<p>A softmax activation takes N numbers $a_1, \ldots, a_{10}$ and converts them to a probability distribution. The first step is to ensure the numbers are positive (since probabilities cannot be negative). This is done by exponentiation.</p>

<script type="math/tex; mode=display">a_i \rightarrow e^{a_i}</script>

<p>The next step is to normalize the numbers i.e. ensure they add up to 1. This is very straightforward. We just divide each score by the sum of scores:</p>

<script type="math/tex; mode=display">p_i = \frac{e^{a_i}}{e^{a_1} + e^{a_2} + \ldots + e^{a_N}}</script>

<p>This is the softmax function. If you have done statistical physics (physics of systems with very large number of interacting constituents), you probably have seen the Boltzmann distribution:</p>

<script type="math/tex; mode=display">p_i = \frac{e^{-\beta E_i}}{e^{-\beta E_1} + e^{-\beta E_2} + \ldots + e^{-\beta E_N}}</script>

<p>which gives the probability that a system with N energy levels is in the state with energy $i$ when it is in equilibrium with a thermal bath at temperature $T = \frac{1}{k_B\beta}$. This is the only probability distribution that is invariant to constant shifts in energy: $E_i \rightarrow E_i + \Delta$.</p>

<p>Network definition</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_ff_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Flatten</span><span class="p">(),</span> 
                             <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
                             <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                             <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
                             <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c">#convert 10-dim activation to probability distribution</span>
                            <span class="p">)</span>
</code></pre></div></div>

<p>Let’s ensure the data flows through our neural network and check the dimensions. As before, the neural net object is a python callable.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_ff_net</span><span class="p">(</span><span class="n">mnist_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">12</span><span class="p">]</span><span class="o">.</span><span class="nb">float</span><span class="p">())</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([12, 10])
</code></pre></div></div>

<p>We get a 10-dimensional output as expected.</p>

<p><strong>Question</strong>: Check that the outputs for each image are actually a probability distribution (the numbers add up to 1).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_ff_net</span><span class="p">(</span><span class="n">mnist_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span><span class="o">.</span><span class="nb">float</span><span class="p">())</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000], grad_fn=&lt;SumBackward2&gt;)
</code></pre></div></div>

<p><strong>Question</strong>: We have an architecture for our neural network but we now need to decide what loss to pick. Unlike the classification problem earlier which had two classes, we have 10 classes here. Take a look at the pytorch documentation - what loss do you think we should pick to model this problem?</p>

<p>We used cross-entropy loss on days 2 and 3. We need the same loss here. Pytorch provides NLLLoss (negative log likelihood) as well as CrossEntropyLoss.</p>

<p><strong>Question</strong>: Look at the documentation for both of these loss functions. Which one should we pick? Do we need to make any modifications to our architecture?</p>

<p>We will use the Cross-entropy Loss which can work with the raw scores (without a softmax layer).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_ff_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Flatten</span><span class="p">(),</span> 
                             <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
                             <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                             <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
                            <span class="p">)</span>
</code></pre></div></div>

<p>Now we’ll get raw unnormalized scores that were used to compute the probabilities. We should use nn.CrossEntropyLoss in this case.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_ff_net</span><span class="p">(</span><span class="n">mnist_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">12</span><span class="p">]</span><span class="o">.</span><span class="nb">float</span><span class="p">())</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ -5.1777,  -0.6201,  25.9891,  -8.4560, -20.0874, -12.2496, -30.4641,
          -5.7812, -29.5144, -25.6087],
        [ -0.9163,  10.6537,  11.8084, -42.2968, -37.6916,  -7.6395,  -0.4226,
          54.1233, -28.6775, -25.0201],
        [  6.6012,   8.0610,  19.0215, -42.6030, -31.1710, -49.4646, -23.2548,
          18.0158, -43.1084, -32.7332],
        [ -0.1542, -11.7846,  27.2837,  10.4074,  -1.8003,  -1.1639, -19.6143,
         -36.8690, -24.0210, -11.8336],
        [ 18.4836,   3.1884,  29.7015, -14.4818, -50.4116,  -1.1841, -37.0231,
          10.3217, -33.0538, -32.8229],
        [ -8.2850,  -2.3705,  32.7147, -27.5788, -19.5514, -16.8190, -39.3709,
          37.0356, -17.2178, -29.3966],
        [ -4.7255,  11.9930,  12.1769, -18.5138, -14.0662, -21.8685,  -1.4869,
           6.5271, -22.4318, -39.7512],
        [ -1.1190, -11.1115,  50.7417,  -5.4773, -19.8561, -24.5303, -17.0328,
           2.3894,  -3.7578, -13.4913],
        [ -3.1977,  -7.0141,  41.0611,   6.5131, -25.9880, -34.2578, -18.2620,
          21.9397, -33.4613, -31.8249],
        [-18.4246, -15.0498,  41.5819,  -1.8826, -31.9494, -19.8510, -18.1790,
           2.5187,  -8.6216, -13.0246],
        [  1.8101, -24.6803,  10.9292, -11.9256,  -4.4042,  -0.5083, -41.4918,
           0.6031, -25.1025, -14.8685],
        [ -2.0029,  -1.1029,  19.0026,  -0.8711,   8.5838,  -4.4992, -15.5333,
         -14.7002, -20.8877, -24.2197]], grad_fn=&lt;AddmmBackward&gt;)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Training</strong>: We have an architecture, the data, an appropriate loss. Now we need to loop over the images, use the loss to compare the predictions to the targets, compute the gradients and update the weights.</p>

<p>In our previous examples, we had N_epoch passes over our dataset and each time, we computed predictions for the full dataset. This is impractical as datasets gets larger. Instead, we need to split the data into <strong>batches</strong> of a fixed size, compute the loss, the gradients and update the weights for each batch.</p>

<p>pytorch provides a DataLoader class that makes it easy to generate batches from your dataset.</p>

<p><strong>Optional</strong>:</p>

<p>Let’s analyze how using batches can be different from using the full dataset. Suppose our data has 10,000 rows but we use batches of size 100 (usually we pick powers of 2 for the GPU but this is just an example). Statistically, our goal is always to compute the gradient:</p>

<script type="math/tex; mode=display">\frac{\partial L}{\partial w_i}</script>

<p>for all the weights $w_i$. By weights here, I mean both the weights and biases and any other free or tunable parameters in our model.</p>

<p>In practice, the loss is a sum over all the examples in our dataset:</p>

<script type="math/tex; mode=display">L = \frac{1}{N}\Sigma_{i}^N l(p_i, t_i)</script>

<p>where $p_i$ = prediction for ith example, $t_i$ = target/label for ith example. So the derivative is:</p>

<script type="math/tex; mode=display">\frac{\partial L}{\partial w_i} = \frac{1}{N}\Sigma_i^N \frac{\partial l(p_i, t_i)}{\partial w_i}</script>

<p>In other words, our goal is to calculate this quantity but $N$ is too large. So we pick a randomly chosen subset of size 100 and only average the gradients over those examples. As an analogy, if our task was to measure the average height of all the people in the world which is impractical, we would pick randomly chosen subsets, say of 10,000 people and measure their average heights.</p>

<p>Of course, as we make the subset smaller, the estimate we get will be noisier i.e. it has a greater chance of higher deviation from the actual value (height or gradient). Is this good or bad? It depends. In our case, we are optimizing a function (the loss) that has multiple local minima and saddle points. It is easy to get stuck in regions of the loss space/surface. Having noisy gradients can help with escaping those local minima just because we’ll not always be moving in the direction of the true gradient but a noisy estimate.</p>

<p>Some commonly used terminology in case you read papers/articles:</p>

<ul>
  <li>
    <p>(Full) Gradient Descent - compute the gradients over the full dataset. Memory-intensive for larger datasets. This is what we did with our toy examples above.</p>
  </li>
  <li>
    <p>Mini-batch Gradient Descent - use randomly chosen samples of fixed size as your data. Noisier gradients, more frequent updates to your model, memory efficient.</p>
  </li>
  <li>
    <p>Stochastic Gradient Descent - Mini-batch gradient descent with batch size = 1. Very noisy estimate, “online” updates to your model, can be hard to converge.</p>
  </li>
</ul>

<p>There are some fascinating papers on more theoretical investigations into the loss surface and the behavior of gradient descent. Here are some examples:</p>

<ul>
  <li>
    <p>https://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets.pdf</p>
  </li>
  <li>
    <p>https://arxiv.org/abs/1811.03804</p>
  </li>
  <li>
    <p>https://arxiv.org/pdf/1904.06963.pdf</p>
  </li>
</ul>

<p><strong>End of optional section</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">16</span> <span class="c">#number of examples to compute gradients over (a batch)</span>


<span class="c">#python convenience classes to sample and create batches</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_train</span><span class="p">,</span> 
                                               <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> 
                                               <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c">#shuffle data</span>
                                               <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                                               <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span>
                                             <span class="p">)</span>
<span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_test</span><span class="p">,</span> 
                                              <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> 
                                              <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c">#shuffle data</span>
                                              <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                                              <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span>
                                             <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data_example</span><span class="p">,</span> <span class="n">target_example</span><span class="p">)</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">data_example</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">target_example</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0
torch.Size([16, 1, 28, 28])
torch.Size([16])
</code></pre></div></div>

<p>So we have batch 0 with 64 tensors of shape (1, 28, 28) and 64 targets. Let’s ensure our network can forward propagate on this batch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_ff_net</span><span class="p">(</span><span class="n">data_example</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

&lt;ipython-input-27-ea1eeb248dd8&gt; in &lt;module&gt;
----&gt; 1 image_ff_net(data_example)


~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    491             result = self._slow_forward(*input, **kwargs)
    492         else:
--&gt; 493             result = self.forward(*input, **kwargs)
    494         for hook in self._forward_hooks.values():
    495             hook_result = hook(self, input, result)


~/.local/lib/python3.7/site-packages/torch/nn/modules/container.py in forward(self, input)
     90     def forward(self, input):
     91         for module in self._modules.values():
---&gt; 92             input = module(input)
     93         return input
     94 


~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    491             result = self._slow_forward(*input, **kwargs)
    492         else:
--&gt; 493             result = self.forward(*input, **kwargs)
    494         for hook in self._forward_hooks.values():
    495             hook_result = hook(self, input, result)


~/.local/lib/python3.7/site-packages/torch/nn/modules/linear.py in forward(self, input)
     90     @weak_script_method
     91     def forward(self, input):
---&gt; 92         return F.linear(input, self.weight, self.bias)
     93 
     94     def extra_repr(self):


~/.local/lib/python3.7/site-packages/torch/nn/functional.py in linear(input, weight, bias)
   1406         ret = torch.addmm(bias, input, weight.t())
   1407     else:
-&gt; 1408         output = input.matmul(weight.t())
   1409         if bias is not None:
   1410             output += bias


RuntimeError: size mismatch, m1: [448 x 28], m2: [784 x 100] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:961
</code></pre></div></div>

<p><strong>Question</strong>: Debug this error</p>

<p>The first shape 1792 x 28 gives us a clue. We want the two 28 sized dimensions to be flattened. But it seems like the wrong dimensions are being flattened here.</p>

<p>1792 = 64 * 28</p>

<p>We need to rewrite our flatten layer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#convert 28x28 image -&gt; 784-dimensional flattened vector</span>
<span class="k">class</span> <span class="nc">Flatten</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Flatten</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">inp</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Flatten</span><span class="p">()(</span><span class="n">data_example</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([16, 784])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_ff_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Flatten</span><span class="p">(),</span> 
                             <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
                             <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                             <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
                            <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_ff_net</span><span class="p">(</span><span class="n">data_example</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([16, 10])
</code></pre></div></div>

<p>Let’s combine all the elements together now and write our training loop.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#convert 28x28 image -&gt; 784-dimensional flattened vector</span>
<span class="k">class</span> <span class="nc">Flatten</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Flatten</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">inp</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    
<span class="c">#ARCHITECTURE</span>
<span class="n">image_ff_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Flatten</span><span class="p">(),</span> 
                             <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
                             <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                             <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
                            <span class="p">)</span>

<span class="c">#LOSS CRITERION and OPTIMIZER</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">image_ff_net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>


<span class="c">#DATALOADERS</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">16</span>

<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_train</span><span class="p">,</span> 
                                               <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> 
                                               <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c">#shuffle data</span>
                                               <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                                               <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span>
                                             <span class="p">)</span>
<span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_test</span><span class="p">,</span> 
                                              <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> 
                                              <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c">#shuffle data</span>
                                              <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                                              <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span>
                                             <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_ff_net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> <span class="c">#don't worry about this (for this notebook)</span>
<span class="n">image_ff_net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">N_EPOCHS</span> <span class="o">=</span> <span class="mi">20</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_EPOCHS</span><span class="p">):</span>
    <span class="n">loss_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data_example</span><span class="p">,</span> <span class="n">data_target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
        <span class="n">data_example</span> <span class="o">=</span> <span class="n">data_example</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">data_target</span> <span class="o">=</span> <span class="n">data_target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">pred</span> <span class="o">=</span> <span class="n">image_ff_net</span><span class="p">(</span><span class="n">data_example</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">data_target</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">loss_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>        
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Epoch = {epoch} Loss = {np.mean(loss_list)}'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch = 0 Loss = 0.4620684621532758
Epoch = 5 Loss = 0.08651585492491722
Epoch = 10 Loss = 0.04343548361460368
Epoch = 15 Loss = 0.05812850828965505
</code></pre></div></div>

<p><strong>Question</strong>: Use your trained network to compute the accuracy on both the train and test sets.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_ff_net</span> <span class="o">=</span> <span class="n">image_ff_net</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span> <span class="c">#don't worry about this (for this notebook)</span>
</code></pre></div></div>

<p>We’ll use argmax to extract the label with the highest probability (or the least negative raw score).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_ff_net</span><span class="p">(</span><span class="n">data_example</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1, 3, 0, 7, 4, 4, 6, 4, 3, 4, 6, 1, 5, 1, 7, 0], device='cuda:0')
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_pred</span><span class="p">,</span> <span class="n">train_targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c">#context manager for inference since we don't need the memory footprint of gradients</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data_example</span><span class="p">,</span> <span class="n">data_target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
        <span class="n">data_example</span> <span class="o">=</span> <span class="n">data_example</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c">#make predictions</span>
        <span class="n">label_pred</span> <span class="o">=</span> <span class="n">image_ff_net</span><span class="p">(</span><span class="n">data_example</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span>
        
        <span class="c">#concat and store both predictions and targets</span>
        <span class="n">label_pred</span> <span class="o">=</span> <span class="n">label_pred</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s">'cpu'</span><span class="p">)</span>
        <span class="n">train_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">train_pred</span><span class="p">,</span> <span class="n">label_pred</span><span class="p">))</span>
        <span class="n">train_targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">train_targets</span><span class="p">,</span> <span class="n">data_target</span><span class="o">.</span><span class="nb">float</span><span class="p">()))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([6., 9., 9., 2., 9., 3., 0., 1., 0., 2.])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_targets</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([6., 9., 9., 2., 9., 3., 0., 1., 0., 2.])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">train_pred</span> <span class="o">==</span> <span class="n">train_targets</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">train_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.993
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>6000
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">assert</span><span class="p">(</span><span class="n">train_pred</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">train_targets</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">train_pred</span> <span class="o">==</span> <span class="n">train_targets</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">train_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Train Accuracy = {train_accuracy:.4f}'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Train Accuracy = 0.9930
</code></pre></div></div>

<p>Here, I want to make an elementary remark about significant figures. While interpreting numbers like accuracy, it is important to realize how big your dataset and what impact flipping one example from a wrong prediction to the right prediction would have.</p>

<p>In our case, the train set has 60,000 examples. Suppose we were to flip one of the incorrectly predicted examples to a correct one (by changing the model, retraining etc etc.). This should change our accuracy, all other examples being the same, by</p>

<script type="math/tex; mode=display">\frac{1}{60,000} = 1.66 * 10^{-5}</script>

<p>Any digits in the accuracy beyond the fifth place have no meaning! For our test set, we have 10,000 examples so we should only care at most about the 4th decimal place (10,000 being a “nice” number i.e. a power of 10 will ensure we never have more any way).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_pred</span><span class="p">,</span> <span class="n">test_targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c">#context manager for inference since we don't need the memory footprint of gradients</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data_example</span><span class="p">,</span> <span class="n">data_target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">):</span>
        <span class="n">data_example</span> <span class="o">=</span> <span class="n">data_example</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c">#make predictions</span>
        <span class="n">label_pred</span> <span class="o">=</span> <span class="n">image_ff_net</span><span class="p">(</span><span class="n">data_example</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span>
        
        <span class="c">#concat and store both predictions and targets</span>
        <span class="n">label_pred</span> <span class="o">=</span> <span class="n">label_pred</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s">'cpu'</span><span class="p">)</span>
        <span class="n">test_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">test_pred</span><span class="p">,</span> <span class="n">label_pred</span><span class="p">))</span>
        <span class="n">test_targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">test_targets</span><span class="p">,</span> <span class="n">data_target</span><span class="o">.</span><span class="nb">float</span><span class="p">()))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">assert</span><span class="p">(</span><span class="n">test_pred</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">test_targets</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">test_pred</span> <span class="o">==</span> <span class="n">test_targets</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">test_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Test  Accuracy = {test_accuracy:.4f}'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Test  Accuracy = 0.9440
</code></pre></div></div>

<p>Great! so our simple neural network already does a great job on our task. At this stage, we would do several things:</p>

<ul>
  <li>
    <p>Look at the examples being classified incorrectly. Are these bad data examples? Would a person also have trouble classifying them?</p>
  </li>
  <li>
    <p>Test stability - what happens if we rotate images? Translate them? Flip symmetric digits? What happens if we add some random noise to the pixel values?</p>
  </li>
</ul>

<p>While we might add these to future iterations of this notebook, let’s move on to some other architectural choices. One of the issues with flattening the input image is that of <strong>locality</strong>. Images have a notion of locality. If a pixel contains part of an object, its neighboring pixels are very likely to contain the same object. But when we flatten an image, we use all the pixels to map to each hidden node in the next layer. If we could impose locality by changing our layers, we might get much better performance.</p>

<p>In addition, we would like image classification to be invariant to certain transformations like translation (move the digit up/down, left/right), scaling (zoom in and out without cropping the image), rotations (at least upto some angular width). Can we impose any of these by our choice of layers?</p>

<p>The answer is yes! Convolutional layers are layers designed specifically to capture such locality and preserve translational invariance. There is a lot of material available describing what these are and we won’t repeat it here. Instead, we’ll repeat the training procedure above but with convolutional layers.</p>

<p>FUTURE TODO: Add analysis of incorrectly predicted examples</p>

<p>FUTURE TODO: add a notebook for image filters, convolutions etc.</p>

<p>Let’s try a convolutional layer:</p>

<p>nn.Conv2d</p>

<p>which takes in the number of input channels (grayscale), number of output channels (we’ll choose 20), kernel size (3x3) and run the transformations on some images.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data_example</span><span class="p">,</span> <span class="n">target_example</span><span class="p">)</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">data_example</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">)(</span><span class="n">data_example</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([16, 1, 28, 28])
torch.Size([16, 20, 26, 26])
</code></pre></div></div>

<p><strong>Question</strong>: If you do know what convolutions are and how filters work, justify these shapes.</p>

<p>The first dimension is the batch size which remains unchanged, as expected. In the raw data, the second dimension is the number of channels i.e. grayscale only and the last two dimensions are the size of the image - 28x28.</p>

<p>We choose 20 channels which explains the output’s second dimension. Each filter is 3x3 and since we have no padding, it can only process 26 patches in each dimension.</p>

<p>If we label the pixels along the columns as 1, 2, …, 28, the patch can be applied from pixels 1-3 (inclusive of both end-points), 2-5, …, 26-28. After that, the patch “falls off” the image unless we apply some padding. This explains the dimension 26 in both directions.</p>

<p>We can then apply a ReLU activation to all these activations.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()((</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">)(</span><span class="n">data_example</span><span class="p">))))</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([16, 20, 26, 26])
</code></pre></div></div>

<p>We should also apply some kind of pooling or averaging now. This reduces noise by picking disjoint, consecutive patches on the image and replacing them by some aggregate statistic like max or mean.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()((</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">)(</span><span class="n">data_example</span><span class="p">)))))</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([16, 20, 13, 13])
</code></pre></div></div>

<p><strong>A couple of notes</strong>:</p>

<ul>
  <li>
    <p>Pytorch’s functions like nn.ReLU() and nn.MaxPool2d() return functions that can apply operations. So, nn.MaxPool2d(kernel_size=2) returns a function that is then applied to the argument above.</p>
  </li>
  <li>
    <p>Chaining together the layers and activations and testing them out like above is very valuable as the first step in ensuring your network does what you want it to do.</p>
  </li>
</ul>

<p>In general, we would suggest the following steps when you are expressing a new network architecture:</p>

<ul>
  <li>
    <p>Build up your network using nn.Sequential if you are just assembling existing or user-defined layers, or by defining a new network class inheriting from nn.Module where you can define a custom forward function.</p>
  </li>
  <li>
    <p>Pick a small tensor containing your features and pass it through each step/layer. Ensure the dimensions of the input and output tensors to each layer make sense.</p>
  </li>
  <li>
    <p>Pick your loss and optimizer and train on a small batch. You should be able to overfit i.e. get almost zero loss on this small set. Neural networks are extremely flexible learners and if you can’t overfit on a small batch, you either have a bug or need to add some more capacity (more nodes, more layers etc. -&gt; more weights).</p>
  </li>
  <li>
    <p>Now you should train on the full train set and practice the usual cross-validation practices.</p>
  </li>
  <li>
    <p>Probe your model: add noise to the inputs, see where the model isn’t performing well, make partial dependency plots etc. to understand characteristics of your model. This part can be very open-ended and it depends on what your final aim is. If you are building a model to predict the stock price so you can trade, you’ll spend a lot of time in this step. If you are having fun predicting dogs vs cats, maybe you don’t care so much. If your aim is to dive deeper into deep learning, looking at the weights, activations, effect of changing hyperparameters, removing edges/weights etc. are very valuable experiments.</p>
  </li>
</ul>

<p>So we have seen one iteration of applying a convolutional layer followed by a non-linearity and then a max pooling layer. We can add more and more of these elements. As you can see, at each step, we are increasing the number of channels increase but the size of the images decreases because of the convolutions and max pooling.</p>

<p><strong>Question</strong>: Feed a small batch through two sequences of Conv -&gt; Relu -&gt; Max pool. What is the output size now?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">data_example</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c">#1 channel in, 16 channels out</span>
<span class="n">out1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()((</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">)(</span><span class="n">data_example</span><span class="p">))))</span>
<span class="k">print</span><span class="p">(</span><span class="n">out1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c">#16 channels in, 32 channels out</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()((</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">)(</span><span class="n">out1</span><span class="p">))))</span>
<span class="k">print</span><span class="p">(</span><span class="n">out2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c">#32 channels in, 128 channels out</span>
<span class="n">out3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()((</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">)(</span><span class="n">out2</span><span class="p">))))</span>
<span class="k">print</span><span class="p">(</span><span class="n">out3</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([16, 1, 28, 28])
torch.Size([16, 16, 13, 13])
torch.Size([16, 32, 5, 5])
torch.Size([16, 128, 1, 1])
</code></pre></div></div>

<p>Recall that we want the output layer to have 10 outputs. We can add a linear/dense layer to do that.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)(</span><span class="n">out3</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

&lt;ipython-input-49-9d722413aea4&gt; in &lt;module&gt;
----&gt; 1 nn.Linear(128, 10)(out3)


~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    491             result = self._slow_forward(*input, **kwargs)
    492         else:
--&gt; 493             result = self.forward(*input, **kwargs)
    494         for hook in self._forward_hooks.values():
    495             hook_result = hook(self, input, result)


~/.local/lib/python3.7/site-packages/torch/nn/modules/linear.py in forward(self, input)
     90     @weak_script_method
     91     def forward(self, input):
---&gt; 92         return F.linear(input, self.weight, self.bias)
     93 
     94     def extra_repr(self):


~/.local/lib/python3.7/site-packages/torch/nn/functional.py in linear(input, weight, bias)
   1406         ret = torch.addmm(bias, input, weight.t())
   1407     else:
-&gt; 1408         output = input.matmul(weight.t())
   1409         if bias is not None:
   1410             output += bias


RuntimeError: size mismatch, m1: [2048 x 1], m2: [128 x 10] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:961
</code></pre></div></div>

<p><strong>Question</strong>: Debug and fix this error. Hint: look at dimensions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)(</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">out3</span><span class="p">))</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([16, 10])
</code></pre></div></div>

<p>It’s time to put all these elements together.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#ARCHITECTURE</span>
<span class="n">image_conv_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
                               <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                               <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
                               
                               <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
                               <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                               <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>

                               <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
                               <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                               <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
                                                  
                               <span class="n">Flatten</span><span class="p">(),</span>
                               <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
                            <span class="p">)</span>

<span class="c">#LOSS CRITERION and OPTIMIZER</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span> <span class="c">#ensure no softmax in the last layer above</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">image_conv_net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>

<span class="c">#DATALOADERS</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">16</span>

<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_train</span><span class="p">,</span> 
                                               <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> 
                                               <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c">#shuffle data</span>
                                               <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                                               <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span>
                                             <span class="p">)</span>
<span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_test</span><span class="p">,</span> 
                                              <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> 
                                              <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c">#shuffle data</span>
                                              <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                                              <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span>
                                             <span class="p">)</span>
</code></pre></div></div>

<p>Train the model. Ideally, write a function so we don’t have to repeat this cell again.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_image_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">loss_criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">N_epochs</span> <span class="o">=</span> <span class="mi">20</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> <span class="c">#don't worry about this (for this notebook)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_epochs</span><span class="p">):</span>
        <span class="n">loss_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data_example</span><span class="p">,</span> <span class="n">data_target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
            <span class="n">data_example</span> <span class="o">=</span> <span class="n">data_example</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">data_target</span> <span class="o">=</span> <span class="n">data_target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data_example</span><span class="p">)</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">data_target</span><span class="p">)</span>

            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="n">loss_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>        
            <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Epoch = {epoch} Loss = {np.mean(loss_list)}'</span><span class="p">)</span>    
            
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_conv_net</span> <span class="o">=</span> <span class="n">train_image_model</span><span class="p">(</span><span class="n">image_conv_net</span><span class="p">,</span>
                                   <span class="n">train_dataloader</span><span class="p">,</span>
                                   <span class="n">criterion</span><span class="p">,</span>
                                   <span class="n">optimizer</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch = 0 Loss = 0.8495251350800196
Epoch = 5 Loss = 0.23720959245165188
Epoch = 10 Loss = 0.18100028269489607
Epoch = 15 Loss = 0.165368815228343
</code></pre></div></div>

<p>Let’s also add a function to do inference and compute accuracy</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict_image_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">):</span>
    <span class="n">pred</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c">#context manager for inference since we don't need the memory footprint of gradients</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data_example</span><span class="p">,</span> <span class="n">data_target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
            <span class="n">data_example</span> <span class="o">=</span> <span class="n">data_example</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="c">#make predictions</span>
            <span class="n">label_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data_example</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span>

            <span class="c">#concat and store both predictions and targets</span>
            <span class="n">label_pred</span> <span class="o">=</span> <span class="n">label_pred</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s">'cpu'</span><span class="p">)</span>
            
            <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">pred</span><span class="p">,</span> <span class="n">label_pred</span><span class="p">))</span>
            <span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">targets</span><span class="p">,</span> <span class="n">data_target</span><span class="o">.</span><span class="nb">float</span><span class="p">()))</span>
            
    <span class="k">return</span> <span class="n">pred</span><span class="p">,</span> <span class="n">targets</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_pred</span><span class="p">,</span> <span class="n">train_targets</span> <span class="o">=</span> <span class="n">predict_image_model</span><span class="p">(</span><span class="n">image_conv_net</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">)</span>
<span class="n">test_pred</span><span class="p">,</span> <span class="n">test_targets</span> <span class="o">=</span> <span class="n">predict_image_model</span><span class="p">(</span><span class="n">image_conv_net</span><span class="p">,</span> <span class="n">test_dataloader</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">assert</span><span class="p">(</span><span class="n">train_pred</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">train_targets</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">train_pred</span> <span class="o">==</span> <span class="n">train_targets</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">train_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Train Accuracy = {train_accuracy:.4f}'</span><span class="p">)</span>

<span class="k">assert</span><span class="p">(</span><span class="n">test_pred</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">test_targets</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">test_pred</span> <span class="o">==</span> <span class="n">test_targets</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">test_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Test  Accuracy = {test_accuracy:.4f}'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Train Accuracy = 0.9653
Test  Accuracy = 0.9240
</code></pre></div></div>

<p>In my case, the test accuracy went from 96.89% to 97.28%. You might see different numbers due to random initialization of weights and different stochastic batches. Is this significant?</p>

<p><strong>Note</strong>: If you chose a small sample of the data, a convolutional neural net might actually do worse than the feed-forward network.</p>

<p><strong>Question</strong>: Do you think the increase in accuracy is significant? Justify your answer.</p>

<p>We have 10,000 examples in the test set. With the feed-forward network, we predicted 9728 examples correctly and with the convolutional net, we predicted 9840 correctly.</p>

<p>We can treat the model as a binomial distribution. Recall the binomial distribution describes the number of heads one gets on a coin which has probability $p$ of giving heads and $1-p$ of giving tails if the coin is tossed $N$ times. More formally, the average number of heads will be:</p>

<script type="math/tex; mode=display">Np</script>

<p>and the standard deviation is:</p>

<script type="math/tex; mode=display">\sqrt{Np(1-p)}</script>

<p>We’ll do a rough back-of-the-envelope calculation. Suppose the true $p$ is what our feed-forward network gave us i.e. $p = 0.9728$ and $N = 10,000$.</p>

<p>Then, the standard deviation is:</p>

<script type="math/tex; mode=display">\sqrt{10000 * 0.9728 * (1-0.9728)} \approx 17</script>

<p>So, to go from 9728 to 9840, we would need ~6.6 standard deviations which is very unlikely. This strongly suggests that the convolutional neural net does give us a significant boost in accuracy as we expected.</p>

<p>You can get a sense of the state-of-the-art on MNIST here: http://yann.lecun.com/exdb/mnist/</p>

<p>Note: MNIST is generally considered a “solved” dataset i.e. it is no longer and hasn’t been for a few years, challenging enough as a benchmark for image classification models. You can check out more datasets (CIFAR, Imagenet etc., MNIST on Kannada characters, fashion MNIST etc.) in torchvision.datasets.</p>

<p><strong>A note about preprocessing</strong>: Image pixels takes values between 0 and 255 (inclusive). In the MNIST data here, all the values are scaled down to be between 0 and 1 by dividing by 255. Often it is helpful to subtract the mean for each pixel to help gradient descent converge faster. As an <strong>exercise</strong>, it is highly encouraged to re-train both the feed-forward and convolutional network with zero-mean images.</p>

<p>Ensure that the means are computed only on the train set and applied to the test set.</p>

<h4 id="autoencoders">Autoencoders</h4>

<p>We have come a long way but there’s still a lot more to do and see. While we have a lot of labelled data, the vast majority of data is unlabelled. There can be various reasons for this. It might be hard to find experts who can label the data or it is very expensive to do so. So another question is whether we can learn something about a dataset without labels. This is a very broad and difficult field called <strong>unsupervised learning</strong> but we can explore it a bit.</p>

<p>Suppose we had the MNIST images but no labels. We can no longer build a classification model with it. But we would still like to see if there are broad categories or groups or clusters within the data. Now, we didn’t cover techniques like K-means clustering this week but they are definitely an option here. Since this is a class on deep learning, we want to use neural networks.</p>

<p>One option is to use networks called <strong>autoencoders</strong>. Since we can’t use the labels, we’ll instead predict the image itself! In other words, the network takes an image as an input and tries to predict it again. This is the identity mapping:</p>

<script type="math/tex; mode=display">i(x) = x</script>

<p>The trick is to force the network to compress the input. In other words, if we have 784 pixels in the input (and the output), we want the hidden layers to use far less than 784 values. Let’s try this.</p>

<p><strong>Note</strong>: I am being sloppy here by pasting the same training code several times. Ideally, I would abstract away the training and inference pieces in functions inside a module.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#convert 28x28 image -&gt; 784-dimensional flattened vector</span>
<span class="c">#redefining for convenience</span>
<span class="k">class</span> <span class="nc">Flatten</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Flatten</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">inp</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AE</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N_input</span><span class="p">,</span> <span class="n">N_hidden_nodes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AE</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Flatten</span><span class="p">(),</span>
                                 <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">N_input</span><span class="p">,</span> <span class="n">N_hidden_nodes</span><span class="p">),</span>
                                 <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                                 <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">N_hidden_nodes</span><span class="p">,</span> <span class="n">N_input</span><span class="p">),</span>
                                 <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
                                <span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
        
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c">#return [BATCH_SIZE, 1, 28, 28]</span>
        
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_ff_ae</span> <span class="o">=</span> <span class="n">AE</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span> <span class="c">#we are choosing 50 hidden activations</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">_</span><span class="p">,</span> <span class="p">(</span><span class="n">data_example</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">data_example</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">image_ff_ae</span><span class="p">(</span><span class="n">data_example</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([16, 1, 28, 28])
torch.Size([16, 1, 28, 28])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">image_ff_ae</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">criterion</span><span class="p">(</span><span class="n">image_ff_ae</span><span class="p">(</span><span class="n">data_example</span><span class="p">),</span> <span class="n">data_example</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(0.2326, grad_fn=&lt;MseLossBackward&gt;)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_image_ae</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">loss_criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">N_epochs</span> <span class="o">=</span> <span class="mi">20</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> <span class="c">#don't worry about this (for this notebook)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_epochs</span><span class="p">):</span>
        <span class="n">loss_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data_example</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
            <span class="c">#Note we don't need the targets/labels here anymore!</span>
            <span class="n">data_example</span> <span class="o">=</span> <span class="n">data_example</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data_example</span><span class="p">)</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">data_example</span><span class="p">)</span>

            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="n">loss_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>        
            <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Epoch = {epoch} Loss = {np.mean(loss_list)}'</span><span class="p">)</span>    
            
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_ff_ae</span> <span class="o">=</span> <span class="n">train_image_ae</span><span class="p">(</span><span class="n">image_ff_ae</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">N_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch = 0 Loss = 0.03667376519739628
Epoch = 5 Loss = 0.01722230292111635
Epoch = 10 Loss = 0.016654866362611452
Epoch = 15 Loss = 0.016621768022576967
</code></pre></div></div>

<p>Let’s look at a few examples of outputs of our autoencoder.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_ff_ae</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s">'cpu'</span><span class="p">)</span>
<span class="n">output_ae</span> <span class="o">=</span> <span class="n">image_ff_ae</span><span class="p">(</span><span class="n">data_example</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">idx</span> <span class="o">=</span> <span class="mi">15</span> <span class="c">#change this to see different examples</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">data_example</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">output_ae</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.image.AxesImage at 0x7fb43bc641d0&gt;
</code></pre></div></div>

<p><img src="output_205_1.png" alt="png" /></p>

<p><img src="output_205_2.png" alt="png" /></p>

<p>So, great - we have a neural network that can predict the input from the input. Is this useful? Recall that we had an intermediate layer that had 50 activations. Feel free to change this number around and see what happens.</p>

<p>We are compressing 784 pixel values into 50 activations and then reconstructing the image from those 50 values. In other words, we are forcing the neural network to capture only relevant non-linear features that can help it remember what image the input was.</p>

<p>The compression is not perfect as you can see in the reconstructed image above but it’s pretty good. Training for more time or better training methods might improve this.</p>

<p>So how exactly is this useful. Maybe:</p>

<ul>
  <li>
    <p>Using an autoencoder to do lossy compression. Image storing the 50 activations instead of each image and storing the last layer (the “decoder”) that constructs the image from the 50 activations.</p>
  </li>
  <li>
    <p>For search: suppose we wanted to search for a target image in a database of N images. We could do N pixel-by-pixel matches but these won’t work because even a slight change in position or orientation or pixel intensities will give misleading distances between images. But if we use the vector of intermediate (50, in this case) activations, then maybe we can do a search in the space of activations. Let’s try that.</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#full mnist data</span>
<span class="k">print</span><span class="p">(</span><span class="n">mnist_train</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([6000, 28, 28])
</code></pre></div></div>

<p>Generally it’s a good idea to split the forward function into an encoder and decoder function. Here we do it explicitly.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_ff_ae</span><span class="o">.</span><span class="n">net</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sequential(
  (0): Flatten()
  (1): Linear(in_features=784, out_features=50, bias=True)
  (2): ReLU()
  (3): Linear(in_features=50, out_features=784, bias=True)
  (4): Sigmoid()
)
</code></pre></div></div>

<p>Compute the activations after the hidden relu</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">mnist_ae_act</span> <span class="o">=</span> <span class="n">image_ff_ae</span><span class="o">.</span><span class="n">net</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">image_ff_ae</span><span class="o">.</span><span class="n">net</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">image_ff_ae</span><span class="o">.</span><span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">mnist_train</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="nb">float</span><span class="p">())))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist_ae_act</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([6000, 50])
</code></pre></div></div>

<p>Let’s pick some example image</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">img_idx</span> <span class="o">=</span> <span class="mi">15</span> <span class="c">#between 0 and 60000-1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">mnist_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">img_idx</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.image.AxesImage at 0x7fb43bb6ddd0&gt;
</code></pre></div></div>

<p><img src="output_214_1.png" alt="png" /></p>

<p>Get the target image activation</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">target_img_act</span> <span class="o">=</span> <span class="n">mnist_ae_act</span><span class="p">[</span><span class="n">img_idx</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">target_img_act</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([   0.0000,    0.0000,  344.1409,    0.0000, 1460.8500,  817.6396,
           0.0000,    0.0000,    0.0000,  780.8979,    0.0000,    0.0000,
         704.9445,    0.0000,    0.0000,  476.1716,  292.7066,    0.0000,
         599.8943,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,
        1457.6251,    0.0000, 2844.3250,    0.0000,    0.0000,    0.0000,
         605.9978,   61.5421,    0.0000, 2161.3923,    0.0000,    0.0000,
           0.0000,    0.0000,  438.2827,  369.7802,    0.0000,    0.0000,
           0.0000,  709.2001,    0.0000,  717.5428,  414.9775,  412.0234,
         483.0129,    0.0000])
</code></pre></div></div>

<p>We will use the cosine distance between two vectors to find the nearest neighbors.</p>

<p><strong>Question</strong>: Can you think of an elegant matrix-operation way of implementing this (so it can also run on a GPU)?</p>

<p><strong>Warning</strong>: Always keep an eye out for memory usage. The full matrix of pairwise distances can be very large. Work with a subset of the data (even 100 images) if that’s the case.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#to save memory, look at only first N images (1000 here)</span>
<span class="n">mnist_ae_act</span> <span class="o">=</span> <span class="n">mnist_ae_act</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1000</span><span class="p">,</span> <span class="p">:]</span>
</code></pre></div></div>

<p>The cosine distance between two points, $\vec{x}_i, \vec{x}_j$ is:</p>

<script type="math/tex; mode=display">d_{ij} = \frac{\vec{x}_i . \vec{x}_j}{\lVert \vec{x}_i \rVert \lVert \vec{x}_j \rVert}</script>

<p>Now we can first normalize all the actiation vector so they have length 1.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">mnist_ae_act</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([1000])
</code></pre></div></div>

<p>We can’t divide a tensor of shape [60000, 50] (activations) by a tensor of shape [60000].</p>

<p>So first we have to unsqueeze (add an additional dimension) to get a shape [60000,1] and then broadcast/expand as the target tensor.</p>

<p>We should check that the first row contains the length of the first image’s activations.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">mnist_ae_act</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">mnist_ae_act</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[14569491., 14569491., 14569491.,  ..., 14569491., 14569491.,
         14569491.],
        [26621508., 26621508., 26621508.,  ..., 26621508., 26621508.,
         26621508.],
        [44020636., 44020636., 44020636.,  ..., 44020636., 44020636.,
         44020636.],
        ...,
        [15814138., 15814138., 15814138.,  ..., 15814138., 15814138.,
         15814138.],
        [19887868., 19887868., 19887868.,  ..., 19887868., 19887868.,
         19887868.],
        [44205368., 44205368., 44205368.,  ..., 44205368., 44205368.,
         44205368.]])
</code></pre></div></div>

<p>Now we can divide by the norm (don’t forget the sqrt).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist_ae_act_norm</span> <span class="o">=</span> <span class="n">mnist_ae_act</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">mnist_ae_act</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">mnist_ae_act</span><span class="p">),</span> <span class="mf">0.5</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s check an example.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist_ae_act</span><span class="p">[</span><span class="mi">10</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([   0.0000,    0.0000,    0.0000,    0.0000, 1373.7821, 1806.4620,
           0.0000,    0.0000,    0.0000, 1539.6263,    0.0000,    0.0000,
         732.2988,    0.0000,    0.0000, 1489.2239,  389.7503,  425.1606,
         990.7882,    0.0000,    0.0000,    0.0000,    0.0000, 1432.6848,
        1173.0520,    0.0000,  909.7496,    0.0000,    0.0000,    0.0000,
           0.0000,  701.1577,    0.0000,  477.3211,    0.0000,    0.0000,
           0.0000,    0.0000, 1125.9907, 1101.0918,    0.0000,    0.0000,
           0.0000, 1666.0519,  129.9131,    0.0000,   12.4162, 1514.1362,
        1475.1205,    0.0000])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">mnist_ae_act</span><span class="p">[</span><span class="mi">10</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">(),</span> <span class="mf">0.5</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(5129.0112)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist_ae_act</span><span class="p">[</span><span class="mi">10</span><span class="p">]</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">mnist_ae_act</span><span class="p">[</span><span class="mi">10</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">(),</span> <span class="mf">0.5</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.2678, 0.3522, 0.0000, 0.0000, 0.0000,
        0.3002, 0.0000, 0.0000, 0.1428, 0.0000, 0.0000, 0.2904, 0.0760, 0.0829,
        0.1932, 0.0000, 0.0000, 0.0000, 0.0000, 0.2793, 0.2287, 0.0000, 0.1774,
        0.0000, 0.0000, 0.0000, 0.0000, 0.1367, 0.0000, 0.0931, 0.0000, 0.0000,
        0.0000, 0.0000, 0.2195, 0.2147, 0.0000, 0.0000, 0.0000, 0.3248, 0.0253,
        0.0000, 0.0024, 0.2952, 0.2876, 0.0000])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist_ae_act_norm</span><span class="p">[</span><span class="mi">10</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.2678, 0.3522, 0.0000, 0.0000, 0.0000,
        0.3002, 0.0000, 0.0000, 0.1428, 0.0000, 0.0000, 0.2904, 0.0760, 0.0829,
        0.1932, 0.0000, 0.0000, 0.0000, 0.0000, 0.2793, 0.2287, 0.0000, 0.1774,
        0.0000, 0.0000, 0.0000, 0.0000, 0.1367, 0.0000, 0.0931, 0.0000, 0.0000,
        0.0000, 0.0000, 0.2195, 0.2147, 0.0000, 0.0000, 0.0000, 0.3248, 0.0253,
        0.0000, 0.0024, 0.2952, 0.2876, 0.0000])
</code></pre></div></div>

<p>Good! They are the same. We have confidence that we are normalizing the activation vectors correctly.</p>

<p>So now the cosine distance is:</p>

<script type="math/tex; mode=display">d_{ij} = \vec{x}_i . \vec{x}_j</script>

<p>since all the vectors are of unit length.</p>

<p><strong>Question</strong>: How would you compute this using matrix operations?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist_ae_act_norm</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([50, 1000])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist_ae_act_norm</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([1000, 50])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ae_pairwise_cosine</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">mnist_ae_act_norm</span><span class="p">,</span> <span class="n">mnist_ae_act_norm</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ae_pairwise_cosine</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([1000, 1000])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ae_pairwise_cosine</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([1000])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">img_idx</span> <span class="o">=</span> <span class="mi">18</span> <span class="c">#between 0 and 60000-1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">mnist_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">img_idx</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Target image"</span><span class="p">)</span>

<span class="c">#find closest image</span>
<span class="n">top5</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">ae_pairwise_cosine</span><span class="p">[</span><span class="n">img_idx</span><span class="p">],</span> <span class="n">descending</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c">#or use argsort</span>
<span class="n">top5_vals</span> <span class="o">=</span> <span class="n">top5</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
<span class="n">top5_idx</span> <span class="o">=</span> <span class="n">top5</span><span class="o">.</span><span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">top5_idx</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">mnist_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Sanity check : same as input"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">f</span><span class="s">"match {i} : cosine = {top5_vals[i]}"</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="output_238_0.png" alt="png" /></p>

<p><img src="output_238_1.png" alt="png" /></p>

<p><img src="output_238_2.png" alt="png" /></p>

<p><img src="output_238_3.png" alt="png" /></p>

<p><img src="output_238_4.png" alt="png" /></p>

<p><img src="output_238_5.png" alt="png" /></p>

<p>While this is a simple dataset and a simple autoencoder, we already have some pretty good anecdotal similarity searches. There are many variations on autoencoders from switching layers to adding noise to the inputs (denoising autoencoders) to adding sparsity penalties to the hidden layer activations to encourage sparse activations to graphical models called variational autoencoders.</p>

<p>Delete activations and cosine distances to save memory</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist_ae_act</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">mnist_ae_act_norm</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">ae_pairwise_cosine</span> <span class="o">=</span> <span class="bp">None</span>
</code></pre></div></div>

<h1 id="conclusion">Conclusion</h1>

<p>By now, you have had quite some experience with writing your own neural networks and introspecting into what they are doing. We still haven’t touched topics like recurrent neural networks, seq2seq models and more modern applications. They will get added to this notebook so if you are interested, please revisit the repo.</p>

<h2 id="future-items">Future Items</h2>

<p>Real problems</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>MNIST + autoencoder (convnet)
</code></pre></div></div>

<p>Trip Classification:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Maybe?
</code></pre></div></div>

<p>RNN toy problems</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Linear trend + noise

Different data structuring strategies

Quadratic trend + noise

LSTM/GRUs for same problems
</code></pre></div></div>

<p>Seq2Seq examples</p>

<p>RNN Autoencoder</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>What data?
</code></pre></div></div>

<h3 id="recurrent-neural-networks-in-progress">Recurrent Neural Networks (In progress)</h3>

<p><strong>Note</strong>: You might have run into memory issues by now. Everything below is self contained so if you want to reset the notebook and start from the cell below, it should work.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span><span class="p">,</span> <span class="n">MLPRegressor</span>

<span class="kn">import</span> <span class="nn">copy</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda:0"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<p>As before, let’s generate some toy data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_rnn_data</span><span class="p">(</span><span class="n">N_examples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">noise_var</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">lag</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">ts</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N_examples</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise_var</span><span class="p">)</span>        
        
    <span class="n">features</span> <span class="o">=</span> <span class="n">ts</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">ts</span><span class="p">)</span><span class="o">-</span><span class="n">lag</span><span class="p">]</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">ts</span><span class="p">[</span><span class="n">lag</span><span class="p">:]</span>
    
    <span class="k">return</span> <span class="n">features</span><span class="p">,</span> <span class="n">target</span>    
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">features</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">generate_rnn_data</span><span class="p">()</span>
</code></pre></div></div>

<p>This data is possibly the simplest time-series one could pick (apart from a constant value). It’s a simple linear trend with a tiny bit of gaussian noise. Note that this is a <strong>non-stationary</strong> series!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="s">'p'</span><span class="p">)</span>
</code></pre></div></div>

<p>We want to predict the series at time t+1 given the value at time t (and history).</p>

<p>Of course, we could try using a feed-forward network for this. But instead, we’ll use this to introduce recurrent neural networks.</p>

<p>Recall that the simplest possible recurrent neural network has a hidden layer that evolves in time, $h_t$, inputs $x_t$ and outputs $y_t$.</p>

<script type="math/tex; mode=display">h_t = \sigma(W_{hh} h_{t-1} + W_{hx} x_t + b_h)</script>

<p>with outputs:</p>

<script type="math/tex; mode=display">y_t = W_{yh} h_t + b_y</script>

<p>Since the output is an unbounded real value, we won’t have an activation on the output.</p>

<p>Let’s write our simple RNN. This is not general - we don’t have the flexibility of adding more layers (as discussed in the lecture), bidirectionality etc. but we are in experimental mode so it’s okay. Eventually, you can use pytorch’s in-built torch.nn.RNN class definition.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N_input</span> <span class="o">=</span> <span class="mi">1</span> <span class="c">#will pass only one value as input</span>
<span class="n">N_output</span> <span class="o">=</span> <span class="mi">1</span> <span class="c">#will predict one value</span>

<span class="n">N_hidden</span> <span class="o">=</span> <span class="mi">32</span> <span class="c">#number of hidden dimensions to use</span>

<span class="n">hidden_activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

<span class="c">#define weights and biases</span>
<span class="n">w_hh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">N_hidden</span><span class="p">,</span> <span class="n">N_hidden</span><span class="p">),</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">w_hx</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">N_hidden</span><span class="p">,</span> <span class="n">N_input</span><span class="p">),</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">w_yh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">N_output</span><span class="p">,</span> <span class="n">N_hidden</span><span class="p">),</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="n">b_h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">N_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">b_y</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">N_output</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="c">#initialize weights and biases (in-place)</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="n">w_hh</span><span class="p">)</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="n">w_hx</span><span class="p">)</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="n">w_yh</span><span class="p">)</span>

<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">b_h</span><span class="p">)</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">b_y</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">hidden_act</span> <span class="o">=</span> <span class="n">hidden_activation</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w_hx</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N_input</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">+</span> \
                               <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w_hh</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">+</span> \
                               <span class="n">b_h</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">hidden_act</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w_yh</span><span class="p">,</span> <span class="n">hidden_act</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<p>But the input we’ll be passing will be a time-series</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inp_ts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">inp_ts</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inp_ts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inp_ts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">hidden_act</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c">#-----------first iter--------</span>
<span class="n">hidden_act</span> <span class="o">=</span> <span class="n">hidden_activation</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w_hx</span><span class="p">,</span> <span class="n">inp_ts</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> \
                               <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w_hh</span><span class="p">,</span> <span class="n">hidden_act</span><span class="p">)</span> <span class="o">+</span> \
                               <span class="n">b_h</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">hidden_act</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w_yh</span><span class="p">,</span> <span class="n">hidden_act</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="c">#-----------second iter--------</span>
<span class="n">hidden_act</span> <span class="o">=</span> <span class="n">hidden_activation</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w_hx</span><span class="p">,</span> <span class="n">inp_ts</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> \
                               <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w_hh</span><span class="p">,</span> <span class="n">hidden_act</span><span class="p">)</span> <span class="o">+</span> \
                               <span class="n">b_h</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">hidden_act</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w_yh</span><span class="p">,</span> <span class="n">hidden_act</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="c">#-----------third iter--------</span>
<span class="n">hidden_act</span> <span class="o">=</span> <span class="n">hidden_activation</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w_hx</span><span class="p">,</span> <span class="n">inp_ts</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> <span class="o">+</span> \
                               <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w_hh</span><span class="p">,</span> <span class="n">hidden_act</span><span class="p">)</span> <span class="o">+</span> \
                               <span class="n">b_h</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">hidden_act</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w_yh</span><span class="p">,</span> <span class="n">hidden_act</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">hidden_act</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inp_ts</span><span class="p">:</span> <span class="c">#input time-series </span>
    <span class="n">hidden_act</span> <span class="o">=</span> <span class="n">hidden_activation</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w_hx</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> \
                                   <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w_hh</span><span class="p">,</span> <span class="n">hidden_act</span><span class="p">)</span> <span class="o">+</span> \
                                   <span class="n">b_h</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">hidden_act</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w_yh</span><span class="p">,</span> <span class="n">hidden_act</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_y</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N_input</span><span class="p">,</span> <span class="n">N_hidden</span><span class="p">,</span> <span class="n">N_output</span><span class="p">,</span> <span class="n">hidden_activation</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">N_input</span> <span class="o">=</span> <span class="n">N_input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N_hidden</span> <span class="o">=</span> <span class="n">N_hidden</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N_output</span> <span class="o">=</span> <span class="n">N_output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_activation</span> <span class="o">=</span> <span class="n">hidden_activation</span>
        
        <span class="c">#define weights and biases</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_hh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">N_hidden</span><span class="p">,</span> <span class="n">N_hidden</span><span class="p">),</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_hx</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">N_hidden</span><span class="p">,</span> <span class="n">N_input</span><span class="p">),</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_yh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">N_output</span><span class="p">,</span> <span class="n">N_hidden</span><span class="p">),</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">b_h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">N_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b_y</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">N_output</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_hh</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_hx</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_yh</span><span class="p">)</span>

        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_h</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_y</span><span class="p">)</span>                        
            
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp_ts</span><span class="p">,</span> <span class="n">hidden_act</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">hidden_act</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c">#initialize to zero if hidden not passed</span>
            <span class="n">hidden_act</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">N_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            

        <span class="n">output_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inp_ts</span><span class="p">:</span> <span class="c">#input time-series </span>
            <span class="n">hidden_act</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_activation</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_hx</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> \
                                                <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_hh</span><span class="p">,</span> <span class="n">hidden_act</span><span class="p">)</span> <span class="o">+</span> \
                                                <span class="bp">self</span><span class="o">.</span><span class="n">b_h</span><span class="p">)</span>

            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_yh</span><span class="p">,</span> <span class="n">hidden_act</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_y</span><span class="p">)</span>
            <span class="n">output_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">output_vals</span><span class="p">,</span> <span class="n">output</span><span class="p">))</span>
            
        <span class="k">return</span> <span class="n">output_vals</span><span class="p">,</span> <span class="n">hidden_act</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">N_input</span><span class="p">,</span> <span class="n">N_hidden</span><span class="p">,</span> <span class="n">N_output</span><span class="p">,</span> <span class="n">hidden_activation</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">output_vals</span><span class="p">,</span> <span class="n">hidden_act</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">inp_ts</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">output_vals</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"---------"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">hidden_act</span><span class="p">)</span>
</code></pre></div></div>

<p>So far so good. Now how do we actually tune the weights? As before, we want to compute a loss between the predictions from the RNN and the labels. Once we have a loss, we can do the usual backpropagation and gradient descent.</p>

<p>Recall that our “features” are:</p>

<script type="math/tex; mode=display">x_1, x_2, x_3\ldots</script>

<p>Our “targets” are:</p>

<script type="math/tex; mode=display">x_2, x_3, x_4 \ldots</script>

<p>if the lag argument in generate_rnn_data is 1. More generally, it would be:</p>

<script type="math/tex; mode=display">x_{1+\text{lag}}, x_{2+\text{lag}}, x_{3+\text{lag}}, \ldots</script>

<p>Now, let’s focus on the operational aspects for a second. In principle, you would first feed $x_1$ as an input, generate an <strong>estimate</strong> for $\hat{x}_2$ as the output.</p>

<p>Ideally, this would be close to the actual value $x_2$ but that doesn’t have to be the case, especially when the weights haven’t been tuned yet. Now, for the second step, we need to input $x_2$ to the RNN. The question is whether we should use $\hat{x}_2$ or $x_2$.</p>

<p>In real-life, one can imagine forecasting a time-series into the future given values till time t. In this case, we would have to feed our prediction at time t, $\hat{x}<em>{t+1}$ as input at the next time-step since we don’t know $x</em>{t+1}$.</p>

<p>The problem with this approach is that errors start compounding really fast. While we might be a bit off at $t+1$, if our prediction $\hat{x}<em>{t+1}$ is inaccurate, then our prediction $\hat{x}</em>{t+2}$ will be even worse and so on.</p>

<p>In our case, we’ll use what’s called <strong>teacher forcing</strong>. We’ll always feed the actual known $x_t$ at time-step t instead of the prediction from the previous time-step, $\hat{x}_t$.</p>

<p><strong>Question</strong>: Split the features and target into train and test sets.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

<span class="n">TRAIN_PERC</span> <span class="o">=</span> <span class="mf">0.70</span>

<span class="n">TRAIN_SPLIT</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">TRAIN_PERC</span> <span class="o">*</span> <span class="n">N_examples</span><span class="p">)</span>

<span class="n">features_train</span> <span class="o">=</span> <span class="n">features</span><span class="p">[:</span><span class="n">TRAIN_SPLIT</span><span class="p">]</span>
<span class="n">target_train</span> <span class="o">=</span> <span class="n">target</span><span class="p">[:</span><span class="n">TRAIN_SPLIT</span><span class="p">]</span>

<span class="n">features_test</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="n">TRAIN_SPLIT</span><span class="p">:]</span>
<span class="n">target_test</span> <span class="o">=</span> <span class="n">target</span><span class="p">[:</span><span class="n">TRAIN_SPLIT</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">features_train</span><span class="p">,</span> <span class="n">features_test</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features_train</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">features_train</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">features_test</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">rnn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N_input</span> <span class="o">=</span> <span class="mi">1</span> <span class="c">#will pass only one value as input</span>
<span class="n">N_output</span> <span class="o">=</span> <span class="mi">1</span> <span class="c">#will predict one value</span>
<span class="n">N_hidden</span> <span class="o">=</span> <span class="mi">32</span> <span class="c">#number of hidden dimensions to use</span>

<span class="n">hidden_activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

<span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">N_input</span><span class="p">,</span> <span class="n">N_hidden</span><span class="p">,</span> <span class="n">N_output</span><span class="p">,</span> <span class="n">hidden_activation</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">features_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">features_train</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">target_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target_train</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="n">features_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">features_test</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">target_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target_test</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">output_vals</span><span class="p">,</span> <span class="n">hidden_act</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">features_train</span><span class="o">.</span><span class="nb">float</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output_vals</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">target_train</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">output_vals</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">(),</span> <span class="n">target_train</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p>We can now put all these ingredients together</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N_input</span> <span class="o">=</span> <span class="mi">1</span> <span class="c">#will pass only one value as input</span>
<span class="n">N_output</span> <span class="o">=</span> <span class="mi">1</span> <span class="c">#will predict one value</span>
<span class="n">N_hidden</span> <span class="o">=</span> <span class="mi">4</span> <span class="c">#number of hidden dimensions to use</span>

<span class="n">hidden_activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>

<span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">N_input</span><span class="p">,</span> <span class="n">N_hidden</span><span class="p">,</span> <span class="n">N_output</span><span class="p">,</span> <span class="n">hidden_activation</span><span class="p">)</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">rnn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">)</span>

<span class="n">N_epochs</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="n">hidden_act</span> <span class="o">=</span> <span class="bp">None</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_epochs</span><span class="p">):</span>
    <span class="n">output_vals</span><span class="p">,</span> <span class="n">hidden_act</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">features_train</span><span class="o">.</span><span class="nb">float</span><span class="p">(),</span> <span class="n">hidden_act</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span>
    
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output_vals</span><span class="p">,</span> <span class="n">target_train</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">())</span>
    
    <span class="c">#loss.requires_grad = True</span>
    
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    
    <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">rnn</span><span class="o">.</span><span class="n">w_yh</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'loss = {loss}'</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">output_vals</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">output_vals</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">criterion</span><span class="p">(</span><span class="n">output_vals</span><span class="p">,</span> <span class="n">target_train</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">features_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">output_vals</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">target_train</span><span class="o">.</span><span class="n">numpy</span><span class="p">()])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rnn</span><span class="o">.</span><span class="n">w_hh</span><span class="o">.</span><span class="n">grad</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rnn</span><span class="o">.</span><span class="n">w_hh</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rnn</span><span class="o">.</span><span class="n">w_hx</span><span class="o">.</span><span class="n">grad</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>
